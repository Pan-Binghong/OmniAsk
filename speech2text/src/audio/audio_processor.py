"""
éŸ³é¢‘å¤„ç†æ¨¡å—ï¼Œè´Ÿè´£éŸ³é¢‘æ•è·å’Œå¤„ç†
"""

import sounddevice as sd
import numpy as np
import threading
import queue
import time
import tempfile
import soundfile as sf
import os
import wave
import requests
import json
import mimetypes
from typing import Optional, List, Dict, Callable, Any
from openai import OpenAI
import psutil
import win32gui
import win32process
from ..config.settings import AUDIO_SETTINGS, WHISPER_SETTINGS, GPT_SETTINGS, QUESTION_KEYWORDS
from ..utils.error_handler import ErrorHandler

# ç”¨äºmultipart/form-dataè¯·æ±‚çš„boundary
BOUNDARY = '----WebKitFormBoundary' + ''.join(['1234567890', 'abcdefghijklmnopqrstuvwxyz'][:10])

# é—®é¢˜å…³é”®è¯
QUESTION_KEYWORDS = ['å—', '?', 'ï¼Ÿ', 'ä»€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¦‚ä½•', 'æ€ä¹ˆ', 'å“ªé‡Œ', 'è°', 'ä½•æ—¶', 'æ˜¯å¦']

class AudioProcessor:
    def __init__(self):
        """åˆå§‹åŒ–éŸ³é¢‘å¤„ç†å™¨"""
        # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
        try:
            self.client = OpenAI(
                api_key=os.getenv('OPENAI_API_KEY'),
                base_url=os.getenv('OPENAI_BASE_URL')
            )
        except Exception as e:
            ErrorHandler.handle_error(e, "APIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥")
            raise
            
        # åˆå§‹åŒ–å†…éƒ¨çŠ¶æ€
        self.audio_queue = queue.Queue()
        self.is_recording = False
        self.text_callback = None
        self.volume_callback = None
        
        # ä»é…ç½®æ–‡ä»¶åŠ è½½è®¾ç½®
        self.sample_rate = AUDIO_SETTINGS['SAMPLE_RATE']
        self.channels = AUDIO_SETTINGS['CHANNELS']
        self.chunk_duration = AUDIO_SETTINGS.get('CHUNK_DURATION', 0.1)
        self.buffer_duration = AUDIO_SETTINGS['BUFFER_DURATION']
        
        self.audio_buffer = []
        self.latest_audio_data = np.array([])
        self.current_device = None
        
    def get_audio_devices(self) -> List[tuple]:
        """è·å–æ‰€æœ‰éŸ³é¢‘è®¾å¤‡"""
        return ErrorHandler.safe_execute(
            self._get_audio_devices,
            "è·å–éŸ³é¢‘è®¾å¤‡æ—¶å‡ºé”™",
            default_return=[]
        )
    
    def _get_audio_devices(self) -> List[tuple]:
        """å†…éƒ¨æ–¹æ³•ï¼šè·å–æ‰€æœ‰éŸ³é¢‘è®¾å¤‡"""
        devices = sd.query_devices()
        device_list = []
        
        # æ·»åŠ è¾“å‡ºè®¾å¤‡
        for i, device in enumerate(devices):
            if device['max_output_channels'] > 0:
                name = f"{device['name']}"
                if device['hostapi'] == 0:  # WASAPI
                    name += " (WASAPI)"
                    device_list.append(("output", i, name))
                elif device['hostapi'] == 1:  # DirectSound
                    name += " (DirectSound)"
                    device_list.append(("output", i, name))
                
        # æ·»åŠ è¾“å…¥è®¾å¤‡
        for i, device in enumerate(devices):
            if device['max_input_channels'] > 0:
                name = f"{device['name']}"
                if device['hostapi'] == 0:  # WASAPI
                    name += " (WASAPI)"
                    device_list.append(("input", i, name))
                elif device['hostapi'] == 1:  # DirectSound
                    name += " (DirectSound)"
                    device_list.append(("input", i, name))
                
        return device_list
        
    def get_audio_applications(self) -> List[tuple]:
        """è·å–æ­£åœ¨æ’­æ”¾éŸ³é¢‘çš„åº”ç”¨ç¨‹åº"""
        return ErrorHandler.safe_execute(
            self._get_audio_applications,
            "è·å–éŸ³é¢‘åº”ç”¨ç¨‹åºæ—¶å‡ºé”™",
            default_return=[]
        )
    
    def _get_audio_applications(self) -> List[tuple]:
        """å†…éƒ¨æ–¹æ³•ï¼šè·å–æ­£åœ¨æ’­æ”¾éŸ³é¢‘çš„åº”ç”¨ç¨‹åº"""
        apps = []
        def enum_windows_callback(hwnd, results):
            if win32gui.IsWindowVisible(hwnd):
                _, pid = win32process.GetWindowThreadProcessId(hwnd)
                try:
                    process = psutil.Process(pid)
                    window_title = win32gui.GetWindowText(hwnd)
                    if window_title and process.name() != "explorer.exe":
                        apps.append((pid, f"{process.name()} - {window_title}"))
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
        
        win32gui.EnumWindows(enum_windows_callback, [])
        return list(set(apps))
        
    def get_available_devices(self) -> List[tuple]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„éŸ³é¢‘è®¾å¤‡å’Œåº”ç”¨ç¨‹åº"""
        devices = self.get_audio_devices()
        for pid, app_name in self.get_audio_applications():
            devices.append(("app", pid, app_name))
        return devices
        
    def audio_callback(self, indata, frames, time, status):
        """éŸ³é¢‘å›è°ƒå‡½æ•°ï¼Œå¤„ç†æ•è·çš„éŸ³é¢‘æ•°æ®"""
        if status:
            print(f"éŸ³é¢‘å›è°ƒçŠ¶æ€: {status}")
        
        # è®¡ç®—å½“å‰éŸ³é‡çº§åˆ«
        if np.any(indata):
            volume_level = np.abs(indata).mean()
            if self.volume_callback:
                self.volume_callback(volume_level)
        
        self.audio_queue.put(indata.copy())
        
    def start_recording(self, device_type: str, device_id: int):
        """å¼€å§‹å½•éŸ³"""
        if self.is_recording:
            # å¦‚æœå·²ç»åœ¨å½•éŸ³ï¼Œå…ˆåœæ­¢å½“å‰å½•éŸ³
            self.stop_recording()
            
        self.is_recording = True
        
        # å¯åŠ¨å½•éŸ³çº¿ç¨‹
        self.record_thread = threading.Thread(
            target=lambda: ErrorHandler.safe_execute(
                self._record_audio,
                "å½•éŸ³è¿‡ç¨‹ä¸­å‡ºé”™",
                self.text_callback,
                device_type=device_type,
                device_id=device_id
            )
        )
        
        # å¯åŠ¨å¤„ç†çº¿ç¨‹
        self.process_thread = threading.Thread(
            target=lambda: ErrorHandler.safe_execute(
                self.process_audio,
                "å¤„ç†éŸ³é¢‘æ—¶å‡ºé”™",
                self.text_callback
            )
        )
        
        self.record_thread.start()
        self.process_thread.start()
            
    def _record_audio(self, device_type: str, device_id: int):
        """å†…éƒ¨æ–¹æ³•ï¼šå®é™…å½•éŸ³å®ç°"""
        try:
            # è·å–è®¾å¤‡ä¿¡æ¯
            devices = sd.query_devices()
            device_info = None
            selected_device_id = device_id  # ä¿å­˜åŸå§‹device_id
            
            if device_type == "app":
                # æŸ¥æ‰¾VB-CABLE Outputè®¾å¤‡
                for i, dev in enumerate(devices):
                    if dev['max_input_channels'] > 0 and ('CABLE Output' in dev['name'] or 'VB-Audio' in dev['name']):
                        selected_device_id = i
                        device_info = dev
                        break
                
                # å¦‚æœæ²¡æ‰¾åˆ°VB-CABLEï¼Œå°è¯•ä½¿ç”¨é»˜è®¤è¾“å…¥è®¾å¤‡
                if device_info is None:
                    default_device = sd.query_devices(kind='input')
                    selected_device_id = default_device['index']
                    device_info = default_device
            else:
                device_info = devices[selected_device_id]
            
            if device_info is None:
                raise Exception("æ‰¾ä¸åˆ°å¯ç”¨çš„éŸ³é¢‘è®¾å¤‡")
            
            # ä¿å­˜å½“å‰è®¾å¤‡ä¿¡æ¯
            self.current_device = device_info
            
            # è®¾ç½®é‡‡æ ·ç‡å’Œé€šé“æ•°
            self.sample_rate = int(device_info['default_samplerate'])
            self.channels = min(2, device_info['max_input_channels'])
            
            # é…ç½®éŸ³é¢‘æµ
            stream_config = {
                'device': selected_device_id,
                'channels': self.channels,
                'samplerate': self.sample_rate,
                'callback': self.audio_callback,
                'blocksize': int(self.sample_rate * self.chunk_duration)
            }
            
            # ä½¿ç”¨WASAPIå…±äº«æ¨¡å¼
            if device_type in ["output", "app"]:
                stream_config['extra_settings'] = dict(
                    wasapi_shared=True,
                    wasapi_exclusive=False
                )
            
            # åˆ›å»ºéŸ³é¢‘æµ
            with sd.InputStream(**stream_config):
                if self.text_callback:
                    self.text_callback(f"æ­£åœ¨ä½¿ç”¨è®¾å¤‡: {device_info['name']}\n")
                    self.text_callback(f"é‡‡æ ·ç‡: {self.sample_rate}Hz, é€šé“æ•°: {self.channels}\n")
                    if device_type == "app":
                        self.text_callback("""
è¯·ç¡®ä¿ï¼š
1. VB-CABLEå·²æ­£ç¡®å®‰è£…å¹¶å¯ç”¨ï¼š
   - åœ¨Windowså£°éŸ³è®¾ç½®ä¸­æ‰¾åˆ°"CABLE Output"
   - ç¡®ä¿è®¾å¤‡å·²å¯ç”¨ä¸”æœªè¢«ç¦ç”¨
   - åœ¨è®¾å¤‡å±æ€§ä¸­å¯ç”¨"ä¾¦å¬æ­¤è®¾å¤‡"
   - é€‰æ‹©æ‚¨çš„è€³æœºä½œä¸ºå›æ”¾è®¾å¤‡

2. éŸ³é¢‘è®¾ç½®æ­£ç¡®ï¼š
   - æ£€æŸ¥ç³»ç»ŸéŸ³é‡æ··åˆå™¨ä¸­çš„åº”ç”¨ç¨‹åºéŸ³é‡
   - ç¡®ä¿åº”ç”¨ç¨‹åºæ­£åœ¨æ’­æ”¾éŸ³é¢‘
   - å°è¯•è°ƒæ•´åº”ç”¨ç¨‹åºçš„éŸ³é¢‘è¾“å‡ºè®¾å¤‡

3. å¦‚æœå¬ä¸åˆ°å£°éŸ³ï¼š
   - æ£€æŸ¥è€³æœºæ˜¯å¦è®¾ä¸ºé»˜è®¤æ’­æ”¾è®¾å¤‡
   - ç¡®ä¿"CABLE Output"çš„ä¾¦å¬åŠŸèƒ½å·²å¯ç”¨
   - æ£€æŸ¥ç³»ç»Ÿå’Œåº”ç”¨ç¨‹åºéŸ³é‡

4. å¦‚æœä»æœ‰é—®é¢˜ï¼š
   - é‡å¯åº”ç”¨ç¨‹åº
   - æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–ç¨‹åºå ç”¨éŸ³é¢‘è®¾å¤‡
   - å°è¯•é‡æ–°æ’æ‹”è€³æœºæˆ–é‡å¯ç”µè„‘
\n""")
                
                while self.is_recording:
                    sd.sleep(int(1000 * self.chunk_duration))
                    
        except Exception as e:
            if not self.is_recording:
                # å¦‚æœå·²ç»ä¸åœ¨å½•éŸ³çŠ¶æ€ï¼Œè¯´æ˜æ˜¯æ­£å¸¸åœæ­¢ï¼Œä¸éœ€è¦æŠ¥é”™
                return
                
            error_msg = f"å½•éŸ³é”™è¯¯: {str(e)}\n"
            if device_type == "app":
                error_msg += f"""
éŸ³é¢‘è®¾å¤‡é…ç½®é”™è¯¯ï¼Œè¯·æ£€æŸ¥ï¼š
1. è®¾å¤‡çŠ¶æ€ï¼š
   - å½“å‰è®¾å¤‡: {self.current_device['name'] if self.current_device else 'æœªçŸ¥'}
   - é‡‡æ ·ç‡: {self.sample_rate}Hz
   - é€šé“æ•°: {self.channels}

2. å¸¸è§é—®é¢˜è§£å†³ï¼š
   - ç¡®ä¿VB-CABLEé©±åŠ¨å·²æ­£ç¡®å®‰è£…
   - æ£€æŸ¥Windowså£°éŸ³è®¾ç½®ä¸­çš„è®¾å¤‡çŠ¶æ€
   - éªŒè¯åº”ç”¨ç¨‹åºçš„éŸ³é¢‘è¾“å‡ºè®¾ç½®
   - å°è¯•é‡æ–°å¯åŠ¨åº”ç”¨ç¨‹åºå’ŒéŸ³é¢‘æœåŠ¡

3. è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼š
{str(e)}
"""
            if self.text_callback:
                self.text_callback(error_msg)
            self.is_recording = False
        
    def stop_recording(self):
        """åœæ­¢å½•éŸ³"""
        if not self.is_recording:
            return
            
        print("æ­£åœ¨åœæ­¢å½•éŸ³...")
        self.is_recording = False
        
        try:
            # ç­‰å¾…çº¿ç¨‹ç»“æŸï¼Œä½†è®¾ç½®è¶…æ—¶æ—¶é—´
            if hasattr(self, 'record_thread'):
                self.record_thread.join(timeout=2.0)
            if hasattr(self, 'process_thread'):
                self.process_thread.join(timeout=2.0)
                
            # æ¸…ç©ºé˜Ÿåˆ—
            while not self.audio_queue.empty():
                try:
                    self.audio_queue.get_nowait()
                except queue.Empty:
                    break
                    
            # é‡ç½®ç¼“å†²åŒºå’Œè®¾å¤‡ä¿¡æ¯
            self.audio_buffer = []
            self.latest_audio_data = np.array([])
            self.current_device = None
            
        except Exception as e:
            ErrorHandler.handle_error(e, "åœæ­¢å½•éŸ³æ—¶å‡ºé”™", self.text_callback)
        finally:
            print("å½•éŸ³å·²åœæ­¢")
            
    def process_audio(self):
        """å¤„ç†éŸ³é¢‘æ•°æ®"""
        buffer_samples = int(self.sample_rate * self.buffer_duration)
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–å‚æ•°
        silence_threshold = AUDIO_SETTINGS.get("SILENCE_THRESHOLD", 0.0015)  # é™éŸ³é˜ˆå€¼ï¼Œå¢åŠ çµæ•åº¦
        min_speech_samples = int(self.sample_rate * AUDIO_SETTINGS.get("MIN_SPEECH_DURATION", 0.5))  # æœ€å°è¯­éŸ³ç‰‡æ®µæ—¶é•¿
        max_speech_samples = int(self.sample_rate * AUDIO_SETTINGS.get("MAX_RECORDING_DURATION", 20.0))  # æœ€å¤§è¯­éŸ³ç‰‡æ®µæ—¶é•¿ï¼ˆæœ€å¤š20ç§’ï¼‰
        speech_energy_threshold = AUDIO_SETTINGS.get("SPEECH_ENERGY_THRESHOLD", 0.002)  # è¯­éŸ³èƒ½é‡é˜ˆå€¼
        
        # è¯­éŸ³çŠ¶æ€
        is_speech = False
        speech_buffer = []
        silence_counter = 0
        # ä»é…ç½®æ–‡ä»¶è¯»å–æœ€å¤§é™éŸ³æ—¶é•¿ï¼Œé»˜è®¤ä¸º1.5ç§’
        max_silence_samples = int(self.sample_rate * AUDIO_SETTINGS.get("PAUSE_TOLERANCE", 1.5))
        
        # é™åˆ¶å›å¤é¢‘ç‡ï¼Œé¿å…é‡å¤æé—®
        last_response_time = 0
        # ä»é…ç½®æ–‡ä»¶è¯»å–æœ€å°å›å¤é—´éš”ï¼Œé»˜è®¤ä¸º2.0ç§’
        min_response_interval = AUDIO_SETTINGS.get("SPEECH_TIMEOUT", 2.0)
        
        while self.is_recording:
            try:
                # ä½¿ç”¨è¶…æ—¶æ¥é¿å…æ— é™ç­‰å¾…
                audio_data = self.audio_queue.get(timeout=0.5)
                
                # å¦‚æœæ˜¯ç«‹ä½“å£°ï¼Œè½¬æ¢ä¸ºå•å£°é“
                if audio_data.shape[1] == 2:
                    audio_data = np.mean(audio_data, axis=1)
                
                # åº”ç”¨ç®€å•çš„å™ªå£°é—¨é™
                audio_data = np.where(np.abs(audio_data) < silence_threshold, 0, audio_data)
                
                # è®¡ç®—å½“å‰å¸§çš„èƒ½é‡
                frame_energy = np.mean(np.abs(audio_data))
                
                if frame_energy > speech_energy_threshold:
                    # æ£€æµ‹åˆ°è¯­éŸ³
                    if not is_speech:
                        is_speech = True
                        speech_buffer = []
                    silence_counter = 0
                    speech_buffer.extend(audio_data)
                else:
                    # æ£€æµ‹åˆ°é™éŸ³
                    if is_speech:
                        silence_counter += len(audio_data)
                        speech_buffer.extend(audio_data)
                        
                        # å¦‚æœé™éŸ³æ—¶é•¿è¶…è¿‡é˜ˆå€¼æˆ–è¯­éŸ³é•¿åº¦è¶…è¿‡æœ€å¤§å€¼ï¼Œå¤„ç†å½“å‰è¯­éŸ³ç‰‡æ®µ
                        if silence_counter >= max_silence_samples or len(speech_buffer) >= max_speech_samples:
                            if len(speech_buffer) >= min_speech_samples:
                                # å¤„ç†è¯­éŸ³ç‰‡æ®µ
                                speech_segment = np.array(speech_buffer)
                                # æ ‡å‡†åŒ–éŸ³é¢‘æ•°æ®
                                speech_segment = speech_segment / (np.max(np.abs(speech_segment)) + 1e-6)
                                # è½¬å†™éŸ³é¢‘
                                text = self.transcribe_audio(speech_segment)
                                if text and self.text_callback:
                                    text = text.strip()
                                    if text and len(text) > 1:  # åªå¤„ç†æœ‰æ„ä¹‰çš„æ–‡æœ¬
                                        current_time = time.time()
                                        is_question_text = self.is_question(text)
                                        
                                        # å¯¹äºé—®é¢˜ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å“åº”
                                        if is_question_text:
                                            # ç¡®ä¿å›å¤é—´éš”å¤§äºæœ€å°é—´éš”
                                            if current_time - last_response_time >= min_response_interval:
                                                self.text_callback(f"é—®é¢˜: {text}\n")
                                                answer = self.get_gpt_response(text)
                                                last_response_time = time.time()  # æ›´æ–°æœ€åå“åº”æ—¶é—´
                                            else:
                                                # å“åº”å¤ªé¢‘ç¹ï¼Œä»…æ˜¾ç¤ºé—®é¢˜
                                                self.text_callback(f"é—®é¢˜: {text} (ç­‰å¾…ä¸­...)\n")
                                        else:
                                            self.text_callback(f"æ–‡æœ¬: {text}\n")
                            # é‡ç½®çŠ¶æ€
                            is_speech = False
                            speech_buffer = []
                            silence_counter = 0
                
            except queue.Empty:
                # é˜Ÿåˆ—è¶…æ—¶ï¼Œç»§ç»­å¾ªç¯
                continue
            except Exception as e:
                ErrorHandler.handle_error(e, "å¤„ç†éŸ³é¢‘æ•°æ®æ—¶å‡ºé”™", self.text_callback)
                    
    def transcribe_audio(self, audio_data: np.ndarray) -> Optional[str]:
        """ä½¿ç”¨OpenAI Whisper APIè½¬å†™éŸ³é¢‘"""
        return ErrorHandler.safe_execute(
            self._transcribe_audio,
            "è½¬å†™éŸ³é¢‘æ—¶å‡ºé”™",
            self.text_callback,
            audio_data=audio_data
        )
    
    def _transcribe_audio(self, audio_data: np.ndarray) -> Optional[str]:
        """å†…éƒ¨æ–¹æ³•ï¼šå®é™…è½¬å†™å®ç°"""
        # å°†éŸ³é¢‘æ•°æ®ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
            sf.write(temp_file.name, audio_data, self.sample_rate)
            temp_filename = temp_file.name

        try:
            # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è¿›è¡ŒéŸ³é¢‘è½¬å†™
            with open(temp_filename, 'rb') as audio_file:
                transcript = self.client.audio.transcriptions.create(
                    model=WHISPER_SETTINGS['model'],
                    file=audio_file,
                    language=WHISPER_SETTINGS.get('language', 'zh'),
                    prompt=WHISPER_SETTINGS.get('prompt', None),
                    response_format="json"
                )
                return transcript.text.strip()
        finally:
            # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
            try:
                os.unlink(temp_filename)
            except Exception as e:
                print(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        
    def is_question(self, text: str) -> bool:
        """æ£€æµ‹æ–‡æœ¬æ˜¯å¦ä¸ºé—®é¢˜"""
        # æ·»åŠ æ›´å¤šçš„åˆ¤æ–­æ¡ä»¶
        
        # 1. å¦‚æœç›´æ¥åŒ…å«é—®å·ï¼Œè‚¯å®šæ˜¯é—®é¢˜
        if "?" in text or "ï¼Ÿ" in text:
            return True
            
        # 2. æ£€æŸ¥å¸¸è§é—®å¥å…³é”®è¯
        question_keywords = QUESTION_KEYWORDS + [
            "è¯·", "èƒ½å¦", "å¯ä»¥", "æ€æ ·", "å¤šå°‘", "å‡ ä¸ª", "æ˜¯ä¸æ˜¯", 
            "æœ‰æ²¡æœ‰", "ä¸ºå•¥", "å’‹", "æœ‰ä½•", "å“ªäº›", "å•¥æ—¶", "å¹²å˜›"
        ]
        
        if any(keyword in text for keyword in question_keywords):
            return True
            
        # 3. æ£€æŸ¥ç‰¹å®šçš„å¥å¼æ¨¡å¼ (ä»¥"è¯·"å¼€å¤´çš„æŒ‡ä»¤)
        if text.strip().startswith("è¯·") and len(text) > 2:
            return True
            
        # 4. å¦‚æœæ–‡æœ¬å¾ˆé•¿(è¶…è¿‡15ä¸ªå­—ç¬¦)ä½†æ²¡æœ‰ä»»ä½•é—®å¥ç‰¹å¾ï¼Œå¯èƒ½æ˜¯é™ˆè¿°å¥
        if len(text) > 15:
            # é•¿æ–‡æœ¬ä½†æ²¡æœ‰æ˜æ˜¾é—®å¥ç‰¹å¾ï¼Œå¯èƒ½æ˜¯é™ˆè¿°æˆ–æè¿°
            return False
            
        # 5. å¦‚æœæ˜¯è¾ƒçŸ­çš„æ–‡æœ¬(<=15å­—ç¬¦)ä¸”æ²¡æœ‰æ˜æ˜¾ç»“æŸç¬¦ï¼Œå€¾å‘äºè®¤ä¸ºæ˜¯é—®é¢˜
        if len(text) <= 15 and not text.endswith(("ã€‚", "ï¼", "~", "â€¦")):
            return True
            
        return False
        
    def get_gpt_response(self, question: str) -> str:
        """è·å–GPTå›ç­”ï¼Œä½¿ç”¨æµå¼è¾“å‡º"""
        return ErrorHandler.safe_execute(
            self._get_gpt_response,
            "è·å–GPTå›ç­”æ—¶å‡ºé”™",
            self.text_callback,
            question=question,
            default_return="æŠ±æ­‰ï¼Œæ— æ³•è·å–å›ç­”ã€‚"
        )
    
    def _get_gpt_response(self, question: str) -> str:
        """å†…éƒ¨æ–¹æ³•ï¼šå®é™…GPTè°ƒç”¨å®ç°"""
        # å…ˆå‘é€æ­£åœ¨å¤„ç†çš„æç¤º
        if self.text_callback:
            self.text_callback("å›ç­”: ")
            self.text_callback("<stream>ğŸ¤” æ­£åœ¨æ€è€ƒ...")
        
        # ä»GPT_SETTINGSä¸­è·å–æ‰€æœ‰å¯ç”¨çš„å‚æ•°
        completion_params = {
            'model': GPT_SETTINGS['model'],
            'messages': [
                {"role": "system", "content": GPT_SETTINGS['system_prompt']},
                {"role": "user", "content": question}
            ],
            'stream': True  # å¯ç”¨æµå¼è¾“å‡º
        }
        
        # å¯é€‰å‚æ•°ï¼Œå¦‚æœåœ¨è®¾ç½®ä¸­å­˜åœ¨åˆ™æ·»åŠ 
        optional_params = ['temperature', 'max_tokens', 'top_p', 
                          'frequency_penalty', 'presence_penalty']
        for param in optional_params:
            if param in GPT_SETTINGS:
                completion_params[param] = GPT_SETTINGS[param]
        
        # ç”¨äºç´¯ç§¯å®Œæ•´çš„å›ç­”
        full_response = ""
        
        # æ¸…é™¤"æ­£åœ¨æ€è€ƒ"æç¤º
        if self.text_callback:
            self.text_callback("<stream>\r" + " " * 20 + "\r")  # æ¸…é™¤å½“å‰è¡Œ
            time.sleep(0.1)  # çŸ­æš‚åœé¡¿
        
        # ä½¿ç”¨æµå¼è°ƒç”¨API
        response = self.client.chat.completions.create(**completion_params)
        
        # é€ä¸ªå¤„ç†æµå¼å“åº”çš„å†…å®¹
        for chunk in response:
            # å®‰å…¨åœ°æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹ï¼Œé¿å…ç´¢å¼•é”™è¯¯
            try:
                if hasattr(chunk, 'choices') and chunk.choices and hasattr(chunk.choices[0], 'delta'):
                    delta = chunk.choices[0].delta
                    if hasattr(delta, 'content') and delta.content:
                        content = delta.content
                        full_response += content
                        # å‘é€å¸¦æœ‰ç‰¹æ®Šæ ‡è®°çš„å¢é‡æ›´æ–°
                        if self.text_callback:
                            self.text_callback(f"<stream>{content}")
                        # å°å»¶è¿Ÿï¼Œé¿å…è¿‡å¿«åˆ·æ–°
                        time.sleep(0.01)
            except (IndexError, AttributeError) as e:
                print(f"å¤„ç†æµå¼å“åº”å—æ—¶å‡ºé”™: {e}")
                continue  # è·³è¿‡è¿™ä¸ªå—ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
        
        return full_response 